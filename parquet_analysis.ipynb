{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tick data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parquet file: EURUSD.parquet\n",
      "CPU times: user 1.42 s, sys: 1.16 s, total: 2.59 s\n",
      "Wall time: 2.59 s\n",
      "=== DATASET INFO ===\n",
      "Shape: (29186310, 5)\n",
      "Memory usage: 3284.44 MB\n",
      "\n",
      "=== COLUMN TYPES ===\n",
      "UTC           object\n",
      "AskPrice     float64\n",
      "BidPrice     float64\n",
      "AskVolume    float64\n",
      "BidVolume    float64\n",
      "dtype: object\n",
      "\n",
      "=== MISSING VALUES ===\n",
      "CPU times: user 779 ms, sys: 12.1 ms, total: 791 ms\n",
      "Wall time: 789 ms\n",
      "No missing values found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "from joblib import Memory\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "path = Path(\"EURUSD.parquet\")\n",
    "# path = Path(\"EURUSD.uncompressed.h5\")\n",
    "# path = Path(\"/media/pete/ramdisk/EURUSD.uncompressed.parquet\")\n",
    "\n",
    "memory = Memory(location=\".cachedir\", verbose=0)\n",
    "\n",
    "@memory.cache\n",
    "def load_data():\n",
    "    return pd.read_parquet(path, engine=\"pyarrow\")\n",
    "\n",
    "if path.exists():\n",
    "    print(f\"Loading parquet file: {path}\")\n",
    "    %time df = load_data()\n",
    "else:\n",
    "    print(f\"File {path} not found!\")\n",
    "\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"=== DATASET INFO ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\n=== COLUMN TYPES ===\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "%time missing_data = df.isnull().sum()\n",
    "print(missing_data[missing_data > 0]) if any(missing_data > 0) else print(\"No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Top and Tail of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 10 rows\n",
    "print(\"=== TOP 10 ROWS ===\")\n",
    "display(df.head(10))\n",
    "\n",
    "# Display bottom 10 rows\n",
    "print(\"\\n=== BOTTOM 10 ROWS ===\")\n",
    "display(df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display statistical summary\n",
    "print(\"=== STATISTICAL SUMMARY ===\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_cols) > 0:\n",
    "    summary = df[numeric_cols].describe()\n",
    "    display(summary)\n",
    "else:\n",
    "    print(\"No numeric columns found for statistical summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Check for unique values in each column\n",
    "print(\"\\n=== UNIQUE VALUES PER COLUMN ===\")\n",
    "for col in df.columns:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{col}: {unique_count} unique values\")\n",
    "    \n",
    "# Check data ranges for numeric columns\n",
    "print(\"\\n=== DATA RANGES FOR NUMERIC COLUMNS ===\")\n",
    "for col in numeric_cols:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    print(f\"{col}: {min_val:.4f} to {max_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for datetime columns\n",
    "datetime_cols = df.select_dtypes(include=['datetime64']).columns\n",
    "if len(datetime_cols) > 0:\n",
    "    print(\"=== TIME SERIES ANALYSIS ===\")\n",
    "    for col in datetime_cols:\n",
    "        print(f\"Column: {col}\")\n",
    "        print(f\"Date range: {df[col].min()} to {df[col].max()}\")\n",
    "        print(f\"Frequency: {len(df)} records over {(df[col].max() - df[col].min()).total_seconds()/3600:.1f} hours\")\n",
    "        print()\n",
    "else:\n",
    "    # Try to parse potential datetime columns\n",
    "    for col in df.columns:\n",
    "        if 'date' in col.lower() or 'time' in col.lower():\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "                print(f\"Converted {col} to datetime\")\n",
    "                print(f\"Date range: {df[col].min()} to {df[col].max()}\")\n",
    "                break\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations for numeric columns\n",
    "if len(numeric_cols) > 1:\n",
    "    print(\"=== CORRELATION MATRIX ===\")\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    display(correlation_matrix)\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions for numeric columns\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"=== DISTRIBUTION PLOTS ===\")\n",
    "    n_cols = min(3, len(numeric_cols))\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        if idx < len(axes):\n",
    "            axes[idx].hist(df[col], bins=30, alpha=0.7, edgecolor='black')\n",
    "            axes[idx].set_title(f'Distribution of {col}')\n",
    "            axes[idx].set_xlabel(col)\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(numeric_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method\n",
    "print(\"=== OUTLIER DETECTION (IQR Method) ===\")\n",
    "outlier_summary = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_pct = (outlier_count / len(df)) * 100\n",
    "    \n",
    "    outlier_summary[col] = {\n",
    "        'count': outlier_count,\n",
    "        'percentage': outlier_pct,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound\n",
    "    }\n",
    "    \n",
    "    print(f\"{col}: {outlier_count} outliers ({outlier_pct:.2f}%)\")\n",
    "\n",
    "# Display outlier details for columns with outliers\n",
    "for col, info in outlier_summary.items():\n",
    "    if info['count'] > 0:\n",
    "        print(f\"\\n{col} outlier bounds: {info['lower_bound']:.4f} to {info['upper_bound']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a comprehensive summary\n",
    "print(\"=== SUMMARY REPORT ===\")\n",
    "print(f\"Dataset: {path}\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    print(f\"\\nNumeric columns: {len(numeric_cols)}\")\n",
    "    for col in numeric_cols:\n",
    "        print(f\"  - {col}: {df[col].mean():.4f} Â± {df[col].std():.4f}\")\n",
    "\n",
    "print(\"\\n=== ANALYSIS COMPLETE ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
